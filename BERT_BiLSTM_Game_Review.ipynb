{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjfCvN5kuraa"
   },
   "source": [
    "## Installing & importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19S4ebapm9hg",
    "outputId": "c2af822d-1ccc-4401-a15f-9e07c44f0359"
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers\n",
    "# ! pip install nltk\n",
    "# ! pip install --upgrade gdown\n",
    "# ! pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NO3IOgDCUrKn"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertModel\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown\n",
    "from tqdm import tqdm, trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWi44a2dpzAm",
    "outputId": "1f79da4f-baf8-4656-f364-9a2e92f8ce79"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all-corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZditjpKEuxYh"
   },
   "source": [
    "Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "O3BzwgXtlZY2",
    "outputId": "268edeb5-dcad-4446-e1b7-75dce988f004"
   },
   "outputs": [],
   "source": [
    "# uri = \"https://drive.google.com/uc?id=1fJujYj3rBuh34FukQa2bG7lhefcYNNwX\"\n",
    "# output = \"dataset/dataset_steam_review.csv\"\n",
    "# if not os.path.exists(\"dataset/\"):\n",
    "#   os.makedirs(\"dataset/\")\n",
    "# gdown.download(url=uri, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vtGunlpuzso"
   },
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5v3clefcPkx",
    "outputId": "90d7f613-8fec-40c6-e0a2-b3b8d9cb0cf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6417106, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_steam_reviews = pd.read_csv(output)\n",
    "df_steam_reviews = pd.read_csv(\"D:/Training/Machine Learning/Datasets/dataset_steam_review/dataset.csv\")\n",
    "df_steam_reviews = df_steam_reviews.sample(frac=1).reset_index(drop=True) #shuffle the data\n",
    "df_steam_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USK4FKFlu2vA"
   },
   "source": [
    "Remove early access reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mM7-05_0lGh_",
    "outputId": "79e56314-1160-4ec6-d147-e87ba11e31b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5392419, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove Early Access Reviews\n",
    "df_steam_reviews = df_steam_reviews[df_steam_reviews.review_text.str.strip() != 'Early Access Review']\n",
    "# size of dataframe\n",
    "df_steam_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n1Fa4ebElULg"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "def remove_links(x):\n",
    "    x = re.sub(r\"http\\S+\", \"\", x)\n",
    "    x = re.sub(r\"https\\S+\", \"\", x)\n",
    "    x = re.sub(r\"www.\\S+\", \"\", x)\n",
    "    x = re.sub(\".*\\..*\\..*\", \"\", x)\n",
    "    return x\n",
    "\n",
    "def remove_hashtag(x):\n",
    "    x = re.sub(\"@[A-Za-z0-9_]+\",\"\", x)\n",
    "    x = re.sub(\"#[A-Za-z0-9_]+\",\"\", x)\n",
    "    return x\n",
    "\n",
    "def remove_punct(x):\n",
    "    x = re.sub(r\"[()!?:;,.'-]\",\"\", x)\n",
    "    return x\n",
    "\n",
    "def remove_emoji(x):\n",
    "    x = x.replace(\":)\", \"\")\n",
    "    x = x.replace(\":-)\", \"\")\n",
    "    x = x.replace(\":(\", \"\")\n",
    "    x = x.replace(\"-_-\", \"\")\n",
    "    x = x.replace(\";)\", \"\")\n",
    "    x = x.replace(\";-)\", \"\")\n",
    "    # REFERENCE FOR EMOJI_PATTERN: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    x = emoji_pattern.sub(r'', x)\n",
    "    return x\n",
    "\n",
    "def remove_titiktitik(x):\n",
    "    x = x.replace(\"..\", \"\")\n",
    "    x = x.replace(\"...\", \"\")\n",
    "    x = x.replace(\"....\", \"\")\n",
    "    x = x.replace(\".....\", \"\")\n",
    "    x = x.replace(\"...................\", \"\")\n",
    "    return x\n",
    "\n",
    "def remove_money(x):\n",
    "    x = re.sub(\"€\", \"\", x)\n",
    "    x = re.sub(\"$\", \"\", x)\n",
    "    x = x.replace(\"usd\", \"\")\n",
    "    return x\n",
    "\n",
    "def fix_typo(x):\n",
    "    x = x.replace(\"veru\", \"very\")\n",
    "    x = x.replace(\"gud\", \"good\")\n",
    "    x = x.replace(\"gut\", \"good\")\n",
    "    x = x.replace(\"withouth\", \"without\")\n",
    "    x = x.replace(\"noob\", \"newbie\")\n",
    "    x = x.replace(\"dis\", \"this\")\n",
    "    x = x.replace(\"noobs\", \"newbie\")\n",
    "    x = x.replace(\"nice1\", \"nice\")\n",
    "    x = x.replace(\"4ever\", \"forever\")\n",
    "    x = x.replace(\"w0n\", \"won\")\n",
    "    x = re.sub(\"&lt;3\", \"\", x)\n",
    "    x = x.replace(\"graficks\", \"graphics\")\n",
    "    x = x.replace(\"dissapeared\", \"disappeared\")\n",
    "    x = x.replace(\"yr\", \"year\")\n",
    "    x = x.replace(\"yrs\", \"years\")\n",
    "    x = x.replace(\"dosent\", \"doesnt\")\n",
    "    x = x.replace(\"awsume\",\"awesome\")\n",
    "    x = re.sub(\"&lt3\",\"\",x)\n",
    "    x = x.replace(\"compatative\", \"competitive\")\n",
    "    x = x.replace(\"cyyounterstrikesyyource\", \"counter strike source\")\n",
    "    x = x.replace(\"&amp\",\"and\")\n",
    "    x = x.replace(\"yyoure\",\"your\")\n",
    "    x = x.replace(\"cyyounter\",\"counter\")\n",
    "    x = x.replace(\"child hood\", \"childhood\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdekX4UrlUtv",
    "outputId": "b2623fd1-6e3f-4f45-954a-3685f8657c8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1965881\n",
       "0     300832\n",
       "Name: review_score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert review text to string\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].astype(str)\n",
    "# convert to lowercase\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(str.lower)\n",
    "\n",
    "# drop the reviews with null score\n",
    "df_steam_reviews = df_steam_reviews[df_steam_reviews[\"review_score\"].notnull()]\n",
    "df_steam_reviews[\"review_score\"] = np.where(df_steam_reviews[\"review_score\"]==-1, 0, df_steam_reviews[\"review_score\"])\n",
    "\n",
    "# remove links\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(remove_links)\n",
    "\n",
    "# remove hashtag\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(remove_hashtag)\n",
    "\n",
    "# removing punctuation\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(remove_punct)\n",
    "\n",
    "# removing dots\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(remove_titiktitik)\n",
    "\n",
    "# removing emoji\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(remove_emoji)\n",
    "\n",
    "# remove money symbols\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(remove_money)\n",
    "\n",
    "# fix any typo\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(fix_typo)\n",
    "\n",
    "# remove any stopwords\n",
    "stop = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_steam_reviews[\"review_text\"] = df_steam_reviews[\"review_text\"].apply(lambda x: \" \".join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "\n",
    "#remove empty text\n",
    "df_steam_reviews = df_steam_reviews[df_steam_reviews.review_text.str.strip() != '']\n",
    "\n",
    "# distribution of negative and positive reviews\n",
    "df_steam_reviews[\"review_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      app_id  count\n",
      "22       570  48575\n",
      "1565  218620  47112\n",
      "1181  105600  46821\n",
      "21       550  31270\n",
      "2156  252950  30497\n",
      "1739  230410  25308\n",
      "5872  391540  21905\n",
      "2582  271590  21651\n",
      "23       620  21038\n",
      "132     4000  17596\n",
      "['Dota 2']\n",
      "['PAYDAY 2']\n",
      "['Terraria']\n",
      "['Left 4 Dead 2']\n",
      "['Rocket League']\n",
      "['Warframe']\n",
      "['Undertale']\n",
      "['Grand Theft Auto V']\n",
      "['Portal 2']\n",
      "[\"Garry's Mod\"]\n"
     ]
    }
   ],
   "source": [
    "# Most reviewed game\n",
    "df2 = df_steam_reviews.groupby(['app_id'])['app_id'].count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "df3 = df2['app_id'].values.tolist()[0:10]\n",
    "\n",
    "print(df2.iloc[:10])\n",
    "# print(df3)\n",
    "\n",
    "for id in df3:\n",
    "    df_gn = df_steam_reviews.loc[df_steam_reviews['app_id'].isin([id])]['app_name'].unique()\n",
    "    print(df_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewed_steam = df_steam_reviews.loc[df_steam_reviews[\"app_id\"].isin(df3)] # take 10 most reviewed game only\n",
    "\n",
    "# sampling the data\n",
    "p = 0.05\n",
    "reviewed_steam = reviewed_steam.sample(frac = p).reset_index(drop=True) # take 5% of data and shuffle it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMwit8Uqu6IQ"
   },
   "source": [
    "Balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfA-tpkRlgU-",
    "outputId": "a16cde1e-c599-4521-e2b3-e9286d3102ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data cleaned: 15589\n",
      "Total data positive: 14073\n",
      "Total data negative: 1516\n"
     ]
    }
   ],
   "source": [
    "# # take the positive as many as the negative ones\n",
    "total_data = len(reviewed_steam[\"review_score\"])\n",
    "total_data_positive = len(reviewed_steam[reviewed_steam[\"review_score\"] == 1])\n",
    "total_data_negative = len(reviewed_steam[reviewed_steam[\"review_score\"] == 0])\n",
    "\n",
    "print(\"Total data cleaned:\", total_data)\n",
    "print(\"Total data positive:\", total_data_positive)\n",
    "print(\"Total data negative:\", total_data_negative)\n",
    "\n",
    "# df_steam_reviews_balanced_positive = reviewed_steam[reviewed_steam[\"review_score\"] == 1].sample(n = total_data_negative) \n",
    "# df_steam_reviews_balanced_negative = reviewed_steam[reviewed_steam[\"review_score\"] == 0] \n",
    "# df_steam_reviews_balanced = pd.concat([df_steam_reviews_balanced_positive, df_steam_reviews_balanced_negative])\n",
    "# df_steam_reviews_balanced = df_steam_reviews_balanced.sample(frac = 1).reset_index(drop=True) #shuffle the data again\n",
    "# df_steam_reviews_balanced.head()\n",
    "\n",
    "# print(\"Data Balanced with ratio 1:1...\")\n",
    "# print(f\"Jumlah data positive : jumlah data negative = {len(df_steam_reviews_balanced_positive)} : {total_data_negative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tgL6jtsmlhAa"
   },
   "outputs": [],
   "source": [
    "# reviews = df_steam_reviews_balanced[\"review_text\"].values.tolist()\n",
    "# labels = df_steam_reviews_balanced[\"review_score\"].tolist()\n",
    "reviews = reviewed_steam[\"review_text\"].values.tolist()\n",
    "labels = reviewed_steam[\"review_score\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15589\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzMUMKZNu9Ay"
   },
   "source": [
    "Preparing data into train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "toY5Q3v-licx"
   },
   "outputs": [],
   "source": [
    "# split the dataset into train, validation and holdout sets (60-20-20)\n",
    "training_sentences, test_sentences, training_labels, test_labels = train_test_split(reviews, labels, test_size=.4)\n",
    "\n",
    "validation_sentences, holdout_sentences, validation_labels, holdout_labels = train_test_split(test_sentences, test_labels, test_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjaUX16UvAz6"
   },
   "source": [
    "## Vector Extraction using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QMInlO0Dl476"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "NUM_LABELS = 2\n",
    "BATCH = 64\n",
    "DEVICE_USED = \"cuda:0\"\n",
    "LEARNING_RATE = 1e-6\n",
    "LAMBDA_L2 = 2e-6\n",
    "EPOCHS = 250\n",
    "MODEL_PATH = \"D:/Training/Machine Learning/NLP/Sentiment Analysis/proposed_model/bilstm_crf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paRGJVHKvtqo",
    "outputId": "dadd5998-b760-484b-fcc9-42d12d231757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE_USED if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3TPT4OWIk-Dl"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('D:/Training/Machine Learning/Datasets/bert-base-uncased', model_max_length=MAX_LEN)\n",
    "# tokenizer = BertTokenizer.from_pretrained('/home/jupyter-23521059/bert-base-uncased', model_max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizeDataset(x_data, y_data):\n",
    "    if len(x_data) != len(y_data):\n",
    "        raise Exception(\"x_data and y_data size are different!\")\n",
    "    \n",
    "    t = trange(len(x_data), colour=\"green\", position=0, leave=True)\n",
    "    \n",
    "    out_padded_token_list = []\n",
    "    out_att_mask = []\n",
    "    out_tok_type_id = []\n",
    "    out_target = []\n",
    "    \n",
    "    for sentence_idx in t:\n",
    "        t.set_description(f\"Tokenizing data [{sentence_idx + 1}/{len(x_data)}]...\")\n",
    "        encoded_sentence = tokenizer.encode_plus(\n",
    "            x_data[sentence_idx],\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            truncation = 'longest_first',\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "\n",
    "        padded_token_list = encoded_sentence['input_ids']\n",
    "        att_mask = encoded_sentence['attention_mask']\n",
    "        tok_type_id = encoded_sentence['token_type_ids']\n",
    "        target = torch.tensor(y_data[sentence_idx])\n",
    "        \n",
    "        out_padded_token_list.append(padded_token_list)\n",
    "        out_att_mask.append(att_mask)\n",
    "        out_tok_type_id.append(tok_type_id)\n",
    "        out_target.append(target)\n",
    "        \n",
    "    output_data = {\n",
    "        \"input_ids\": out_padded_token_list,\n",
    "        \"attention_mask\": out_att_mask,\n",
    "        \"token_type_ids\": out_tok_type_id,\n",
    "        \"label\": out_target\n",
    "    }\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data [9353/9353]...: 100%|\u001b[32m█████████████████████████████████████████████\u001b[0m| 9353/9353 [00:10<00:00, 888.89it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenized_training = TokenizeDataset(training_sentences, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data [3118/3118]...: 100%|\u001b[32m█████████████████████████████████████████████\u001b[0m| 3118/3118 [00:03<00:00, 898.66it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenized_validation = TokenizeDataset(validation_sentences, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data [3118/3118]...: 100%|\u001b[32m█████████████████████████████████████████████\u001b[0m| 3118/3118 [00:03<00:00, 907.80it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenized_holdout = TokenizeDataset(holdout_sentences, holdout_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5a_lYyHu1Qu",
    "outputId": "a5608b5e-c25a-495d-b40c-8c6be98864d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/Training/Machine Learning/Datasets/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\n",
    "    'D:/Training/Machine Learning/Datasets/bert-base-uncased', \n",
    "#     '/home/jupyter-23521059/bert-base-uncased',\n",
    "    num_labels=NUM_LABELS,\n",
    "    output_hidden_states = True, # Whether the model returns all hidden-states,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Vd3l0WeAxgyD"
   },
   "outputs": [],
   "source": [
    "# # Put model to GPU\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWAy1EYGve6P"
   },
   "source": [
    "Extract the embedding weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedData(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.att_mask = attention_mask\n",
    "        self.tti = token_type_ids\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.att_mask[idx],\n",
    "            \"token_type_ids\": self.tti[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_train_tok = TokenizedData(tokenized_training['input_ids'], tokenized_training['attention_mask'], tokenized_training['token_type_ids'], tokenized_training['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_valid_tok = TokenizedData(tokenized_validation['input_ids'], tokenized_validation['attention_mask'], tokenized_validation['token_type_ids'], tokenized_validation['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_holdout_tok = TokenizedData(tokenized_holdout['input_ids'], tokenized_holdout['attention_mask'], tokenized_holdout['token_type_ids'], tokenized_holdout['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_training_tok = DataLoader(\n",
    "    datas_train_tok,\n",
    "    batch_size = BATCH\n",
    ")\n",
    "dataloader_valid_tok = DataLoader(\n",
    "    datas_valid_tok,\n",
    "    batch_size = BATCH\n",
    ")\n",
    "dataloader_holdout_tok = DataLoader(\n",
    "    datas_holdout_tok,\n",
    "    batch_size = BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(\n",
    "#         input_ids = tokenized_training['input_ids'][0],\n",
    "#         attention_mask = tokenized_training['attention_mask'][0],\n",
    "#         token_type_ids = tokenized_training['token_type_ids'][0]\n",
    "#     )\n",
    "    \n",
    "#     last_hidden_state = outputs[0]\n",
    "#     hidden_states = outputs[2]\n",
    "#     initial_embedding = hidden_states[0] # initial embedding\n",
    "#     word_embed_4_last_layers = torch.stack(hidden_states[-4:]).sum(0) #sum of last 4 hidden layers\n",
    "    \n",
    "#     print(\"last hidden state:\", last_hidden_state.size())\n",
    "#     print(\"hidden states:\", len(hidden_states))\n",
    "#     print(\"initial embedding:\", initial_embedding.size())\n",
    "#     print(\"last 4 layers:\", word_embed_4_last_layers.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractEmbedding(the_model, datas, total_data):\n",
    "    the_model.eval() # put model to evaluation mode\n",
    "    \n",
    "    bert_embedding_sv = []\n",
    "    bert_embedding_label = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        t = tqdm(enumerate(datas), colour=\"green\", position=0, leave=True, total=len(datas))\n",
    "        i = 0\n",
    "        for batch, data in t:\n",
    "            for idx in range(len(data[\"input_ids\"])):\n",
    "                in_ids = data[\"input_ids\"][idx]\n",
    "                att_mask = data[\"attention_mask\"][idx]\n",
    "                tok_type = data[\"token_type_ids\"][idx]\n",
    "\n",
    "                output = the_model(\n",
    "                    input_ids = in_ids,\n",
    "                    attention_mask = att_mask,\n",
    "                    token_type_ids = tok_type\n",
    "                )\n",
    "                \n",
    "                hidden_states = output[2]\n",
    "                word_embed_4_last_layers = torch.stack(hidden_states[-4:]).sum(0) #sum of last 4 hidden layers\n",
    "#                 print(\"last 4 layers:\", word_embed_4_last_layers.size())\n",
    "#                 print(\"label:\", data['label'][idx])\n",
    "                \n",
    "                bert_embedding_sv.append(word_embed_4_last_layers)\n",
    "                bert_embedding_label.append(data['label'][idx])\n",
    "                \n",
    "                t.set_description(f\"Extracting embedding weight [{i+1}/{total_data}] \")\n",
    "                t.refresh()\n",
    "                \n",
    "                i += 1\n",
    "        return bert_embedding_sv, bert_embedding_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embedding weight [9353/9353] : 100%|\u001b[32m██████████████████████████████████████\u001b[0m| 147/147 [35:43<00:00, 14.58s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "training_embeddings, training_embd_labels = ExtractEmbedding(model, dataloader_training_tok, len(tokenized_training['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embedding weight [3118/3118] : 100%|\u001b[32m████████████████████████████████████████\u001b[0m| 49/49 [12:21<00:00, 15.12s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "validation_embeddings, valid_embd_labels = ExtractEmbedding(model, dataloader_valid_tok, len(tokenized_validation['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embedding weight [3118/3118] : 100%|\u001b[32m████████████████████████████████████████\u001b[0m| 49/49 [12:30<00:00, 15.32s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "holdout_embeddings, holdout_embd_labels = ExtractEmbedding(model, dataloader_holdout_tok, len(tokenized_holdout['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang embedding: 9353, Panjang label: 9353\n"
     ]
    }
   ],
   "source": [
    "print(f\"Panjang embedding: {len(training_embeddings)}, Panjang label: {len(training_embd_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_embeddings[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Model: BiLSTM + CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for sentence in reviews:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "\n",
    "words = list(set(words))\n",
    "VOCAB_LEN = len(words)\n",
    "print(VOCAB_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedModel1(torch.nn.Module):\n",
    "    def __init__(self, lstm_in_size, lstm_hdn_size, lstm_layers, lstm_dropout = 0.2):\n",
    "        super(ProposedModel1, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_hdn_size = lstm_hdn_size\n",
    "        \n",
    "        self.batchnorm = torch.nn.BatchNorm1d(lstm_in_size)\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = lstm_in_size,\n",
    "            hidden_size = lstm_hdn_size//2,\n",
    "            num_layers = lstm_layers,\n",
    "            bidirectional = True,\n",
    "            dropout = lstm_dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features = lstm_hdn_size,\n",
    "            out_features = 768 # number of features inside hidden layer\n",
    "        )\n",
    "        self.fc2 = torch.nn.Linear(\n",
    "            in_features = 768,\n",
    "            out_features = 2 # number of classes (binary)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_perm = x.permute(0, 2, 1)\n",
    "        b = self.batchnorm(x_perm)\n",
    "        b_perm = b.permute(0, 2, 1)\n",
    "\n",
    "        # Init the hidden state\n",
    "        h0 = torch.zeros(2*self.lstm_layers, x.size(0), self.lstm_hdn_size//2).requires_grad_()\n",
    "        h0 = h0.to(device)\n",
    "      \n",
    "        # Init the cell state\n",
    "        c0 = torch.zeros(2*self.lstm_layers, x.size(0), self.lstm_hdn_size//2).requires_grad_()\n",
    "        c0 = c0.to(device)\n",
    "\n",
    "        h, _ = self.lstm(b_perm, (h0.detach(), c0.detach()))\n",
    "        y1 = self.fc1(h)\n",
    "        output = self.fc2(y1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedModel2(torch.nn.Module):\n",
    "    def __init__(self, gru_in_size, gru_hdn_size, gru_layers, gru_dropout):\n",
    "        super(ProposedModel2, self).__init__()\n",
    "        \n",
    "        self.gru_layers = gru_layers\n",
    "        self.gru_hdn_size = gru_hdn_size\n",
    "        \n",
    "        self.batchnorm = torch.nn.BatchNorm1d(gru_in_size)\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size = gru_in_size,\n",
    "            hidden_size = gru_hdn_size//2,\n",
    "            num_layers = gru_layers,\n",
    "            bidirectional = True,\n",
    "            dropout = gru_dropout,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features = gru_hdn_size,\n",
    "            out_features = 768 # number of features inside hidden layer\n",
    "        )\n",
    "        self.fc2 = torch.nn.Linear(\n",
    "            in_features = 768,\n",
    "            out_features = 2 # number of classes (binary)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_perm = x.permute(0,2,1)\n",
    "        b = self.batchnorm(x_perm)\n",
    "        b_perm = b.permute(0,2,1)\n",
    "        \n",
    "        # Init the hidden state\n",
    "        h0 = torch.zeros(2*self.gru_layers, x.size(0), self.gru_hdn_size//2).requires_grad_()\n",
    "        h0 = h0.to(device)\n",
    "\n",
    "        h, _ = self.gru(b_perm, h0.detach())\n",
    "        y1 = self.fc1(h)\n",
    "        output = self.fc2(y1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProposedModel1(\n",
      "  (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(768, 512, num_layers=4, batch_first=True, dropout=0.25, bidirectional=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=768, bias=True)\n",
      "  (fc2): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "proposed_model1 = ProposedModel1(\n",
    "    lstm_in_size = 768,\n",
    "    lstm_hdn_size = 1024,\n",
    "    lstm_layers = 2,\n",
    "    lstm_dropout = 0.45\n",
    ")\n",
    "proposed_model1 = proposed_model1.to(device)\n",
    "print(proposed_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProposedModel2(\n",
      "  (batchnorm): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gru): GRU(768, 512, num_layers=4, batch_first=True, dropout=0.25, bidirectional=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=768, bias=True)\n",
      "  (fc2): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "proposed_model2 = ProposedModel2(\n",
    "    gru_in_size = 768,\n",
    "    gru_hdn_size = 1024,\n",
    "    gru_layers = 2,\n",
    "    gru_dropout = 0.35\n",
    ")\n",
    "proposed_model2 = proposed_model2.to(device)\n",
    "print(proposed_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn1 = torch.nn.BCELoss()\n",
    "\n",
    "# Define optimizer\n",
    "opt1 = torch.optim.AdamW(\n",
    "    proposed_model1.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=LAMBDA_L2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn2 = torch.nn.BCELoss()\n",
    "\n",
    "# Define optimizer\n",
    "opt2 = torch.optim.AdamW(\n",
    "    proposed_model2.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=LAMBDA_L2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all needed functions to train, validate, and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader for embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, arr_embed, arr_lbl):\n",
    "        super(EmbeddingDataset, self).__init__()\n",
    "        self.array_embed = arr_embed\n",
    "        self.array_label = arr_lbl\n",
    "    def __len__(self):\n",
    "        return len(self.array_embed)\n",
    "    def __getitem__(self, idx):\n",
    "        all_embedding = self.array_embed[idx][0, :, :] # torch.squeeze(self.array_embed[idx])\n",
    "        cls_embedding = self.array_embed[idx][0, 0, :]\n",
    "        cls_embedding = cls_embedding[None, :]\n",
    "        data_pair = {\n",
    "            \"all_embedding\": all_embedding, # all embedding data (all of seq length) - size [128, 768]\n",
    "            \"cls_embedding\": cls_embedding, # CLS embedding only - size [1, 768]\n",
    "            \"label\": self.array_label[idx]\n",
    "        }\n",
    "        return data_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_dataset = EmbeddingDataset(training_embeddings, training_embd_labels)\n",
    "embed_valid_dataset = EmbeddingDataset(validation_embeddings, valid_embd_labels)\n",
    "embed_holdout_dataset = EmbeddingDataset(holdout_embeddings, holdout_embd_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_dataloader = DataLoader(\n",
    "    embed_train_dataset,\n",
    "    batch_size = BATCH\n",
    ")\n",
    "embed_valid_dataloader = DataLoader(\n",
    "    embed_valid_dataset,\n",
    "    batch_size = BATCH\n",
    ")\n",
    "embed_holdout_dataloader = DataLoader(\n",
    "    embed_holdout_dataset,\n",
    "    batch_size = BATCH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(true_labels, predicted_labels):\n",
    "    total_acc = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    total_f1 = f1_score(true_labels, predicted_labels)\n",
    "    returned_dict = {\n",
    "        \"total_accuracy\": total_acc,\n",
    "        \"total_f1_score\": total_f1\n",
    "    }\n",
    "    return returned_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(embed_dataloader, the_model, loss_func, optimizer):\n",
    "    the_model.train()\n",
    "    \n",
    "    concat_true_lbl = []\n",
    "    concat_pred_lbl = []\n",
    "    \n",
    "    for batch, datas in enumerate(embed_dataloader):\n",
    "        embed = datas['all_embedding']\n",
    "        lbl = datas['label']\n",
    "        \n",
    "        embed = embed.to(device)\n",
    "        lbl = lbl.to(device)\n",
    "        \n",
    "        # FF\n",
    "        output = the_model(embed)\n",
    "        y_pred = output[:,0,0]\n",
    "        y_pred = y_pred.type(torch.float)\n",
    "        y_pred1 = output[:,0,:].to('cpu').detach()\n",
    "        y_pred2 = np.argmax(y_pred1, axis=1).to(device).float().requires_grad_()\n",
    "        \n",
    "        # calculate loss\n",
    "        lbl_float = lbl.float()\n",
    "        loss = loss_func(y_pred2, lbl_float)\n",
    "        \n",
    "        # calculate metrics\n",
    "        ground_truth = lbl_float.to('cpu').detach()\n",
    "        ground_truth = ground_truth.int()\n",
    "        predicted_lbl = y_pred2.to('cpu').detach()\n",
    "        predicted_lbl = predicted_lbl.int()\n",
    "        \n",
    "        concat_true_lbl += ground_truth\n",
    "        concat_pred_lbl += predicted_lbl\n",
    "        \n",
    "        # backpro\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # calculate metrics\n",
    "    calculated_metric = calc_metric(concat_true_lbl, concat_pred_lbl)\n",
    "    \n",
    "    returned_loss = loss.item()\n",
    "    returned_loss /= BATCH\n",
    "    \n",
    "    return calculated_metric, returned_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(embed_dataloader, the_model, loss_func):\n",
    "    the_model.eval()\n",
    "    \n",
    "    concat_true_lbl = []\n",
    "    concat_pred_lbl = []\n",
    "    with torch.no_grad():\n",
    "        for batch, datas in enumerate(embed_dataloader):\n",
    "            embed = datas['all_embedding']\n",
    "            lbl = datas['label']\n",
    "\n",
    "            embed = embed.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "\n",
    "            # FF\n",
    "            output = the_model(embed)\n",
    "            y_pred = output[:,0,0]\n",
    "            y_pred = y_pred.type(torch.float)\n",
    "            y_pred1 = output[:,0,:].to('cpu').detach()\n",
    "            y_pred2 = np.argmax(y_pred1, axis=1).to(device).float().requires_grad_()\n",
    "\n",
    "            # calculate loss\n",
    "            lbl_float = lbl.float()\n",
    "            loss = loss_func(y_pred2, lbl_float)\n",
    "\n",
    "            ground_truth = lbl_float.to('cpu').detach()\n",
    "            predicted_lbl = y_pred2.to('cpu').detach()\n",
    "            \n",
    "            concat_true_lbl += ground_truth\n",
    "            concat_pred_lbl += predicted_lbl\n",
    "            \n",
    "    # calculate metrics\n",
    "    calculated_metric = calc_metric(concat_true_lbl, concat_pred_lbl)\n",
    "    \n",
    "    returned_loss = loss.item() \n",
    "    returned_loss /= BATCH\n",
    "    \n",
    "    return calculated_metric, returned_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_sequence(epoch, training_device, proposed_model, training_dataloader, validation_dataloader, loss_fn, opt, saved_model_path, saved_model_name = \"model.pt\", use_early_stopping = False, patience = 3, min_delta = 10):\n",
    "    proposed_model = proposed_model.to(training_device)\n",
    "    \n",
    "    # Make tqdm progress bar\n",
    "    t = trange(epoch, position=0, leave=True, colour=\"green\")\n",
    "    \n",
    "    history_chart = {\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"valid_accuracy\": [],\n",
    "        \"valid_f1\": [],\n",
    "        \"valid_loss\": []\n",
    "    }\n",
    "    \n",
    "    early_stopper = EarlyStopper(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for ep in t:\n",
    "        # Train the model\n",
    "        train_metric, train_loss = train_model(training_dataloader, proposed_model, loss_fn, opt)\n",
    "        \n",
    "        # Measure loss and accuracy\n",
    "        valid_metric, valid_loss = inference(validation_dataloader, proposed_model, loss_fn)\n",
    "\n",
    "        t.set_description(f\"Train loss: {train_loss:>.4f} train acc: {train_metric['total_accuracy']:>.2f}, val loss: {valid_loss:>.4f} val acc: {valid_metric['total_accuracy']:>.2f}\")\n",
    "\n",
    "        # Add to history to be plotted\n",
    "        history_chart[\"train_accuracy\"].append(train_metric['total_accuracy'])\n",
    "        history_chart[\"train_f1\"].append(train_metric[\"total_f1_score\"])\n",
    "        history_chart[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        history_chart[\"valid_accuracy\"].append(valid_metric['total_accuracy'])\n",
    "        history_chart[\"valid_f1\"].append(valid_metric[\"total_f1_score\"])\n",
    "        history_chart[\"valid_loss\"].append(valid_loss)\n",
    "        \n",
    "        # Save model\n",
    "        if(valid_metric[\"total_f1_score\"] > max(history_chart[\"valid_f1\"])):\n",
    "            if saved_model_path[-1] == \"/\":\n",
    "                os.makedirs(saved_model_path[:-1], exist_ok=True)\n",
    "                if saved_model_name[-3:] == \".pt\" or saved_model_name[-4:] == \".pth\":\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + saved_model_name)\n",
    "                else:\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + saved_model_name + \".pt\")\n",
    "            else:\n",
    "                os.makedirs(saved_model_path, exist_ok=True)\n",
    "                if saved_model_name[-3:] == \".pt\" or saved_model_name[-4:] == \".pth\":\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + \"/\" + saved_model_name)\n",
    "                else:\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + \"/\" + saved_model_name + \".pt\")\n",
    "        elif(len(history_chart[\"valid_f1\"]) == 1):\n",
    "            if saved_model_path[-1] == \"/\":\n",
    "                os.makedirs(saved_model_path[:-1], exist_ok=True)\n",
    "                if saved_model_name[-3:] == \".pt\" or saved_model_name[-4:] == \".pth\":\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + saved_model_name)\n",
    "                else:\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + saved_model_name + \".pt\")\n",
    "            else:\n",
    "                os.makedirs(saved_model_path, exist_ok=True)\n",
    "                if saved_model_name[-3:] == \".pt\" or saved_model_name[-4:] == \".pth\":\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + \"/\" + saved_model_name)\n",
    "                else:\n",
    "                    torch.save(proposed_model.state_dict(), saved_model_path + \"/\" + saved_model_name + \".pt\")\n",
    "\n",
    "        if early_stopper.early_stop(valid_metric[\"total_f1_score\"]) and use_early_stopping == True:\n",
    "            break\n",
    "    return history_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lets_plot(training_value, validation_value, y_caption, title, background_color='black'):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1) # nrows, ncols, index\n",
    "    ax.set_facecolor(background_color)\n",
    "    plt.plot(training_value, color='red', label='Train')\n",
    "    if validation_value != None:\n",
    "        plt.plot(validation_value, color='yellow', label='Valid')\n",
    "        plt.legend(['Train', 'Valid'], loc='upper right')\n",
    "    else:\n",
    "        plt.legend(['Train'], loc='upper right')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_caption)\n",
    "    plt.grid(color='white', linestyle='--', linewidth=0.5)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3472 train acc: 0.50, val loss: 0.2378 val acc: 0.50:  76%|\u001b[32m███████▋  \u001b[0m| 191/250 [1:15:41<23:22, 23.78s/it]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_with_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposed_model1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_valid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/bilstm/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_bilstm.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 20\u001b[0m, in \u001b[0;36mtraining_sequence\u001b[1;34m(epoch, training_device, proposed_model, training_dataloader, validation_dataloader, loss_fn, opt, saved_model_path, saved_model_name, use_early_stopping, patience, min_delta)\u001b[0m\n\u001b[0;32m     16\u001b[0m early_stopper \u001b[38;5;241m=\u001b[39m EarlyStopper(patience\u001b[38;5;241m=\u001b[39mpatience, min_delta\u001b[38;5;241m=\u001b[39mmin_delta)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     train_metric, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Measure loss and accuracy\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     valid_metric, valid_loss \u001b[38;5;241m=\u001b[39m inference(validation_dataloader, proposed_model, loss_fn)\n",
      "Cell \u001b[1;32mIn[48], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(embed_dataloader, the_model, loss_func, optimizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m lbl \u001b[38;5;241m=\u001b[39m lbl\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# FF\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mthe_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m output[:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[38], line 39\u001b[0m, in \u001b[0;36mProposedModel1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_hdn_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m     37\u001b[0m c0 \u001b[38;5;241m=\u001b[39m c0\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 39\u001b[0m h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_perm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(h)\n\u001b[0;32m     41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(y1)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:769\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    773\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_with_lstm = training_sequence(EPOCHS, device, proposed_model1, embed_train_dataloader, embed_valid_dataloader, loss_fn1, opt1, MODEL_PATH + \"/bilstm/\", \"model_bilstm.pt\", True, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_with_gru = training_sequence(EPOCHS, device, proposed_model2, embed_train_dataloader, embed_valid_dataloader, loss_fn2, opt2, MODEL_PATH + \"/bigru/\", \"model_bigru.pt\", True, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bilstm_metric, test_bilstm_loss = inference(embed_holdout_dataloader, proposed_model1, loss_fn1)\n",
    "print(f\"Test accuracy with BiLSTM: {test_bilstm_metric['total_accuracy']} - Test F1 Score with BiLSTM: {test_bilstm_metric['total_f1_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gru_metric, test_gru_loss = inference(embed_holdout_dataloader, proposed_model2, loss_fn2)\n",
    "print(f\"Test accuracy with BiGRU: {test_gru_metric['total_accuracy']} - Test F1 Score with BiGRU: {test_gru_metric['total_f1_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot BiLSTM model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_plot(history_with_lstm[\"train_accuracy\"], history_with_lstm[\"valid_accuracy\"], \"\", \"Accuracy with BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_plot(history_with_lstm[\"train_f1\"], history_with_lstm[\"valid_f1\"], \"\", \"F1 Score with BiLSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_plot(history_with_lstm[\"train_loss\"], history_with_lstm[\"valid_loss\"], \"\", \"Loss with BiLSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot BiGRU model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_plot(history_with_gru[\"train_accuracy\"], history_with_gru[\"valid_accuracy\"], \"\", \"Accuracy with BiGRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_plot(history_with_gru[\"train_f1\"], history_with_gru[\"valid_f1\"], \"\", \"F1 Score with BiGRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_plot(history_with_gru[\"train_loss\"], history_with_gru[\"valid_loss\"], \"\", \"Loss with BiGRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jjfCvN5kuraa"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "my-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
